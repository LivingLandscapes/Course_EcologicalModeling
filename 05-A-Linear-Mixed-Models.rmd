---
layout: topic
title: "Linear Mixed Models"
output: html_document
---

**Assigned Reading:**

- Harrison XA, Donaldson L, Correa-Cano ME, Evans J, Fisher DN, Goodwin CED, Robinson BS, Hodgson DJ, Inger R. 2018. A brief introduction to mixed effects modelling and multi-model inference in ecology. PeerJ 6:e4794 https://doi.org/10.7717/peerj.4794.
- Oberpriller, J., de Souza Leite, M., & Pichler, M. (2022). Fixed or random? On the reliability of mixed‐effects models for a small number of levels in grouping variables. Ecology and Evolution, 12(7), e9062.

```{r include = FALSE}

# This code block sets up the r session when the page is rendered to html
# include = FALSE means that it will not be included in the html document

# Write every code block to the html document 
knitr::opts_chunk$set(echo = TRUE)

# Write the results of every code block to the html document 
knitr::opts_chunk$set(eval = TRUE)

# Set the web address where R will look for files from this repository
# Do not change this address
repo_url <- "https://raw.githubusercontent.com/LivingLandscapes/Course_EcologicalModeling/master/"

# Suppress warnings and messages
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

```

# Overview

We will be covering linear mixed models in this lab using two different datasets:

-  First, we will use the 'sleepstudy' dataset that comes loaded with the lme4 package. From the help documentation: *The average reaction time per day for subjects in a sleep deprivation study. On day 0 the subjects had their normal amount of sleep. Starting that night they were restricted to 3 hours of sleep per night. The observations represent the average reaction time on a series of tests given each day to each subject.*
-  Second, we will use a 'mice' dataset, which was collected by January Frost at the University of Nebraska. The mice dataset contains morphological measurements of mice caught at various sites in the Niobrara Valley.

```{r, include = TRUE, eval = TRUE, message = FALSE, results = 'hide'}

# List of packages necessary to run this script:
require(librarian, quietly = TRUE)
shelf(tidyverse, cowplot, performance, 
      AICcmodavg, # for model selection
      lme4, # For mixed modeling
      lmerTest,
      pander,
      broom.mixed,
      lib = tempdir(),
      quiet = TRUE)

# Set the web address where R will look for files from this repository
# Do not change this address
repo_url <- "https://raw.githubusercontent.com/LivingLandscapes/Course_EcologicalModeling/master/"

# Load our two datasets
# NOTE: 'sleepstudy' data comes loaded in lme4
mice <- read.table(paste0(repo_url, "/data/mice.txt"), 
                   header = TRUE)

```

## Sleep Study

### Simple linear regression

First, let's simply model the response of reaction time to number of days without sleep. **On your own:**

- Do some quick data exploration (e.g., familiarize yourself with the data columns, their ranges, etc.)
- Interpret the results of the linear regression below.

```{r eval = TRUE, message = FALSE, include = TRUE}

# Simple linear regression between reaction time and number of days without
# sleep.
ss.lm <- lm(Reaction ~ Days, sleepstudy)
# summary(ss.lm)

```

Hopefully, you will have determined that (unsurprisingly) reaction time tends to slow as days without sleep increased.

However, you will also hopefully have noticed that there were several "subjects" of the experiment and that each subject's response times were measured multiple times... Do we think there's any variation between subject response times? Let's check it out:

```{r, eval = TRUE, message = FALSE, include = TRUE}

ssBase <- 
  ggplot(sleepstudy, 
         aes(x = Days, 
             y = Reaction)) + 
  geom_point(aes(color = Subject)) + 
  scale_color_viridis_d() + 
  labs(x="Days with no sleep",
       y="Reaction time [ms]") + 
  geom_smooth(method = "lm", formula = y ~ x)
ssBase

```

Yep, look like some variation to me! Let's plot the relationship for each subject individually and see what happens:

```{r, eval = TRUE, message = FALSE, include = TRUE}

# group by subject
ssBase + 
  facet_wrap( ~ Subject) + 
  guides(color = FALSE) # suppress legend because facet labels do the job

```

If the relationship is plotted separately for each individual in the dataset, then the signal gets much stronger. Clearly, people differ in their responses to sleep deprivation. One subject got faster as they missed out on sleep!

We could fit a separate model to each subject and combine them. This is effectively a Subject * Days interaction model.

```{r, eval = TRUE, message = FALSE, include = TRUE}

# Fit a separate linear regression to each subject
fm1 <- lme4::lmList(Reaction ~ Days | Subject, sleepstudy)

# Get the coefficients from the list of linear regressions:
coef(fm1)

```

The `lmList()` function fits a different linear model to each subset of a data.frame defined by the variable after the “|”, in this case “Subject”. 

**On your own:**

- Calculate the mean intercept and Days coefficients from the matrix created by `coef(fm1)`. 
- Compare these with the estimates from the single regression model. 
- Use `summary(fm1)` to see the residual variance and degrees of freedom for the pooled list. Compare these values with the residual variance and degrees of freedom from the single regression model.

Ask Caleb to confirm your answers once you've completed this!

```{r eval=TRUE, message=FALSE, include=FALSE}

# Coefficients from linear regression list by subject
cc <- coef(fm1)

# Are coefficients from the simple model and list of models equal?
all.equal(sapply(cc, mean), # mean of list model coefficients
          coef(ss.lm)) # coefficients from simple linear regression

```

### Mix it up!

We can use a mixed model approach to get the best of both worlds - an estimate of the population average intercept and slope, and an idea of how much variation there is among individuals. The function lmer() in package lme4 is the method of choice here. The fixed effects part of the formula is identical to the lm() formula, with a response variable on the left of the ˜ and a covariate on the right. The new part specifies the random effects, and these are given in (). *To the left of the | are the fixed effects that will vary among groups, and to the right of the | are the variable(s) that specify the groups.* In this case the variable Subject defines the groups. As with other formulas, the intercept is implicit, so (Days | Subject) indicates that both the intercept and the slope are allowed to vary among subjects. This is the same as fitting a separate line to each subject, as we did above.

```{r eval = TRUE, message = FALSE, include = TRUE}

# fit a linear mixed model
ss.mm <- 
  lmer(Reaction ~ Days + (Days | Subject),
       sleepstudy,
       REML = FALSE) # NOTE: you have to specify REML vs ML in lmer
# summary(ss.mm)

```

Looking at the summary of the mixed model, you should see a “fixed effects” section which is identical to that from a normal lm() fit, with one notable exception. There are no p-values reported for each coefficient. This isn’t a mistake, it is a deliberate choice by the software designer to avoid an argument over the correct number of degrees of freedom to use for the test! 

**On your own, compare the estimates and standard errors with those from the "lmList" and "lm" models above.**

The new bit is the section labelled “Random effects”. We can see that the model has estimated not just one variance, but three, plus a correlation coefficient. One variance estimate is the residual variance, with the same interpretation as always. The other two variances indicate the variance of the “random effects” on the intercept and the slope. These random perturbations to the population are assumed to be correlated to some extent, so there is also a correlation parameter. In this case the random effects are relatively uncorrelated.

We can also get the random perturbations for each group, and combining those with the population level coefficients gives us the coefficients for the line in each group, as shown in the following table. When the random effects (shown in the left 2 columns) are negative, the coefficients for the group are less than the population coefficients, and when they are positive, the group coefficients are greater.

```{r eval = TRUE, message = FALSE, include = TRUE}

knitr::kable(cbind(ranef(ss.mm)$Subject, coef(ss.mm)$Subject))

```

In the following figure I’ve plotted the intercepts for each group from the mixed model against the intercepts from the list of models. If they were identical they would fall on the solid line. 

```{r eval = TRUE, message = FALSE, include = TRUE}

shrink <-  
  data.frame(coef(ss.mm)$Subject %>%
               rename(Random_Intercept = `(Intercept)`,
                      Random_Days = Days), 
             cc %>%
               rename(Fixed_Intercept = `(Intercept)`,
                      Fixed_Days = Days))

t1 <-
  shrink %>%
  pivot_longer(names(shrink), 
               values_to = "estimate",
               names_to = "coefficient") %>%
  separate_wider_delim(cols = "coefficient", 
                       names = c("model", "coefficient"),
                       delim = "_") %>%
  group_by(model) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = "model", 
              values_from = "estimate")

# phew! that was more work than I expected. 

# make a little dataframe for the average point
meanpoint <- 
  data.frame(coefficient = c("Intercept","Days"),
                        Random=fixef(ss.mm),
                        Fixed=coef(ss.lm))

# now make the plot ... easier b/c of data manipulation?
ggplot(t1, 
       aes(x = Fixed,
               y = Random)) +
  geom_point() +
  facet_wrap(~ coefficient, 
             scales = "free") + 
  geom_smooth(method = "lm") + 
  geom_abline(slope = 1, 
              intercept = 0, 
              linetype = 2) + 
  geom_point(data = meanpoint, 
             color = "violetred1",
             size = 2)

```

Random effects coefficients are slightly biased towards the mean value indicated by the red dot. Therefore the blue line (fit to the points) has a slope less than 1. The dashed line has a slope of 1 and an intercept of 0. This phenomenon
is called “shrinkage”, because the estimates of the within group coefficients “shrink” towards the population mean.

## Mouse Example 

Let's look at the breakdown of the relationship between ear size and foot length by geographic site. Sex may also matter?

```{r eval = TRUE, message = FALSE, include = TRUE, fig.cap='Ear length vs. foot length in mm at a number of sites in the Northwest of Nebraska for two species of deer mouse.'}

# Remove rows with unknown sex.
mice = filter(mice, sex != "U")

# Plot relationship
basemouse <- 
  ggplot(mice,
         aes(x = foot, y = ear)) +
  geom_point(alpha = 0.2) +
  xlab('Foot Length [mm]') + ylab('Ear Length [mm]')

basemouse +
  geom_smooth(method = "lm") +
  facet_wrap( ~ site)
   
```

A few things are obvious from this plot. First, there is a lot of data at a few sites, and much less at others. Some of the sex size effects are clear in a few sites (two groups of points), but much less clear in others. But most interesting is the variation in slope. In many sites the bigger the feet the bigger the ears. But not everywhere! Put it all in one plot, adjusting the alpha level so overplotting is clearer.

```{r eval = TRUE, message = FALSE, include = TRUE, fig.cap='Ear length by foot length with a predicted line for each site. Shading indicates overlapping points.'}

 basemouse +
   geom_smooth(aes(group = site),
               method = "lm", 
               se = FALSE)

```

So we want to check a model that lets the relationship between feet and ears vary by species and sex. The sites are a random sample of possible places we could look for mice, so we'll treat site as a random effect. We can also see that the slope of the effect varies alot between sites, so we'll let the coefficient of foot vary between sites too, at least initially. Here's the global model, and check the residuals:

```{r, eval = TRUE, message = FALSE, include = TRUE}

# 
mice.global <- 
  lmer(ear ~ foot * species * sex + (foot | site),
       data = mice)

rr <- broom::augment(mice.global)

ggplot(rr, 
       aes(x = .fitted, 
           y = .resid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, 
             linetype = 2)

# get int and slope for qqline
probs <- c(0.25,
           0.75)
y <- quantile(rr$.resid, probs, names = FALSE, na.rm = TRUE)
x <- qnorm(probs)
slope <- diff(y) / diff(x)
int <- y[1L] - slope * x[1L]
ggplot(rr, 
       aes(sample = .resid)) +
  geom_qq(size = rel(4)) +
  geom_abline(
    intercept = int,
    slope = slope,
    linetype = 2,
    size = 2
  )

ggplot(rr,
       aes(x = .fitted,
           y = sqrt(abs(.resid)))) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept  = 1)

```

The residuals don't look too bad, although there are some large positive residuals which we'll ignore for now. So let's fit a range of models with different random effects structures, and compare using AICc. The problem with comparing different random effects using Likelihood Ratio tests is that our null hypothesis is "on the boundary" of the parameter space, because you can't have negative variances. Choosing a model that has the minimum AICc won't be bothered by this consideration, but this would affect the calculation of the weights.

```{r eval = TRUE, message = FALSE, include = TRUE}

# uncorrelated random effects # NOTE: Does not converge!
mice.1 <-
  lmer(ear ~ foot * species * sex + (foot - 1 | site) + (1 | site),
       data = mice)

# only random slope
mice.2 <- 
  lmer(ear ~ foot * species * sex + (foot - 1 | site), 
       data = mice)

# only random intercept
mice.3 <- 
  lmer(ear ~ foot * species * sex + (1 | site), 
       data = mice)

# Model selection for random effects.
# NOTE: AICcmodavg and MuMIn functions won't work because these models are
# different "classes". Check it out if you're interested:
# lapply(list(mice.global, mice.1, mice.2, mice.3), class)
aicc <- sapply(list(mice.global, mice.1, mice.2, mice.3), AICc)
knitr::kable(
  cbind(
    Model = c("Correlated", "Uncorrelated", "Slope", "Intercept"),
    AICc = format(aicc, digits = 5
    )
))

```

We're in luck, the model with both a random slope and intercept where the effects are correlated has the lowest AICc, and by a large margin. 

However, we still need to select the best *fixed effect* structure too! To do that, we need to create some candidate models based off the global fixed effects while always using the random effect structure we identified. And critically, we need to **use Maximum Likelihood to compare fixed effects**.

On your own, 

Our final model is simply `ear ~ foot + (1+foot|site)`. This means the relationship between ear and foot length is constant between sexes and species, but varies geographically. I find that quite fascinating. I'll fit this final model, and make a plot or two.

```{r eval = TRUE, message = FALSE, include = TRUE}
mice.final <- 
  lmer(ear ~ foot + (foot | site),
       data = mice,
       REML = FALSE)
# summary(mice.final)
```

You can make predictions from these models just as you would with other models. To use `broom::augment()` you have to specify both the fixed effects values *and* a value for the random effect.

```{r eval = TRUE, message = FALSE, include = TRUE}
# nd <- data.frame(foot=seq(min(mice$foot),
#                           max(mice$foot),length=50),
#                  site=factor("42", levels=levels(mice$site)))
# 
# pred42 <- 
#   broom.mixed::augment(mice.final, newdata=nd) %>%
#   rename(ear=.fitted)
# 
# ggplot(mice, aes(x=foot, y=ear)) +
#   geom_point(alpha=0.2) +
#   xlab('Foot Length [mm]') + ylab('Ear Length [mm]') +
#   geom_point(data=filter(mice, site=="42"), alpha=1, color="red") +
#   geom_line(data=pred42, size=2, color="red")
```

So you could get lines for every site, but that would be a bit messy. We say these predictions are "conditional" on the random effects, because they include the perturbations from the random effects. Another thing you might want are predictions "unconditional" on the random effects. We can get these too but not from `broom::augment()`. Package lme4 includes a `predict()` function for these fitted objects.

```{r eval = TRUE, message = FALSE, include = TRUE}

# # get predictions unconditional on random effect. Note the 're.form' argument.
# preduc <- cbind(nd, ear=predict(mice.final, newdata = nd, re.form = ~0))
# 
# # make the plot above, but fade out the red line and add unconditional prediction
# ggplot(mice, aes(x=foot, y=ear)) +
#   geom_point(alpha=0.2) +
#   xlab('Foot Length [mm]') + ylab('Ear Length [mm]') +
#   geom_line(data=pred42, size=2, color="red", alpha=0.4) +
#   geom_line(data=preduc, size=2)
```
That line represents the relationship between ears and feet we expect to find at a randomly chosen new site.

What about confidence intervals on our predictions? A very good question, young grasshopper, but one that will have to wait!